{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether or not running on Kaggle\n",
    "import os\n",
    "KAGGLE_RUN = True\n",
    "N_TRAIN_ROWS = 10 * 1000000\n",
    "if os.path.isdir('/home/ubuntu'):\n",
    "    KAGGLE_RUN = False\n",
    "    \n",
    "if KAGGLE_RUN:\n",
    "    model_name = 'model8/model8.txt'\n",
    "else:\n",
    "    model_name = 'model8.txt'\n",
    "    \n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "if not KAGGLE_RUN:\n",
    "    import sys\n",
    "    sys.path.insert(0, './input')\n",
    "    import local_work\n",
    "import riiideducation\n",
    "\n",
    "# Boto3 Setup and Download Files\n",
    "if not KAGGLE_RUN:\n",
    "    local_work = local_work.local_work()\n",
    "    local_work.download_riiid_files()\n",
    "    \n",
    "# Column definitions    \n",
    "train_cols = ['user_id', 'timestamp', 'task_container_id'\n",
    "              ,'content_id', 'content_type_id', 'answered_correctly'\n",
    "             ,'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if not KAGGLE_RUN:\n",
    "    df_train = local_work.get_train_data(train_cols, nrow=N_TRAIN_ROWS)\n",
    "    df_questions = local_work.get_questions_data()\n",
    "else:\n",
    "    import datatable as dt\n",
    "    if N_TRAIN_ROWS is None:\n",
    "        df_train = dt.fread('../input/riiid-test-answer-prediction/train.csv'\n",
    "                            ,columns=set(train_cols)).to_pandas()        \n",
    "    else:\n",
    "        df_train = dt.fread('../input/riiid-test-answer-prediction/train.csv'\n",
    "                            ,columns=set(train_cols)\n",
    "                            ,max_nrows=N_TRAIN_ROWS).to_pandas()\n",
    "    df_questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv'\n",
    "                                ,usecols=[0, 3]\n",
    "                                ,dtype={'question_id': 'int16', 'part': 'int8'}\n",
    "                               )\n",
    "    \n",
    "\n",
    "# Cleaning function\n",
    "def clean(df, target, df_questions, is_train):\n",
    "    # Only keep question rows\n",
    "    df = df[df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    # Assume False if NA (Only for first question ? )\n",
    "    df['prior_question_had_explanation'].fillna(False, inplace=True)  \n",
    "    \n",
    "    # Alter columns types\n",
    "    tmp = {'user_id': 'int32'\n",
    "            ,'content_id': 'int16'\n",
    "            ,target: 'int8'\n",
    "            ,'prior_question_elapsed_time': 'float32' \n",
    "            ,'prior_question_had_explanation': 'bool'\n",
    "            }\n",
    "    if not is_train:\n",
    "        del tmp[target]\n",
    "    df = df.astype(tmp)\n",
    "    \n",
    "    # Merge questions\n",
    "    df = pd.merge(df, df_questions, left_on='content_id', right_on='question_id', how='left')\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "# Feature Engineering function\n",
    "def do_fe(df\n",
    "          ,is_train\n",
    "          ,content_agg=None\n",
    "          ,user_sum_dict=None\n",
    "          ,user_count_dict=None\n",
    "          ,content_sum_dict=None\n",
    "          ,content_count_dict=None\n",
    "          ,user_prev_task_sum_dict=None\n",
    "          ,user_prev_task_count_dict=None\n",
    "          ,user_prev_task_timestamp_dict=None\n",
    "         ):\n",
    "\n",
    "    if is_train:\n",
    "        # Get sum/count of answers in current, previous, and cumulative tasks.\n",
    "        df_tasks = df\\\n",
    "                    .groupby(['user_id', 'task_container_id'], sort=False)[target]\\\n",
    "                    .agg(['sum', 'count'])\\\n",
    "                    .rename(columns={'sum': 'task_sum', 'count': 'task_count'})\n",
    "        df_tasks['prev_task_sum'] = df_tasks.groupby('user_id')['task_sum'].shift()\n",
    "        df_tasks['prev_task_count'] = df_tasks.groupby('user_id')['task_count'].shift()\n",
    "        df_tasks['prev_task_cum_sum'] = df_tasks.groupby('user_id')['prev_task_sum'].cumsum()\n",
    "        df_tasks['prev_task_cum_count'] = df_tasks.groupby('user_id')['prev_task_count'].cumsum()\n",
    "        \n",
    "        # FE accuracy on previous task(s)\n",
    "        df_tasks['prev_task_correctness'] = df_tasks['prev_task_sum']/df_tasks['prev_task_count']\n",
    "        df_tasks['prev_task_cum_correctness'] = df_tasks['prev_task_cum_sum']/\\\n",
    "                                                    df_tasks['prev_task_cum_count']\n",
    "\n",
    "        # Drop undesired columns\n",
    "        df_tasks.drop(columns=['prev_task_sum', 'prev_task_count', 'prev_task_cum_sum'], inplace=True)\n",
    "        \n",
    "        # Join df_tasks to df_train\n",
    "        df = df.merge(df_tasks, on=['user_id', 'task_container_id'])\n",
    "\n",
    "        # Garbage College\n",
    "        del df_tasks\n",
    "        gc.collect()\n",
    "        \n",
    "        # Add time since last task\n",
    "        df_tasks = df_train\\\n",
    "                    .groupby(['user_id', 'task_container_id'], sort=False)[['timestamp']]\\\n",
    "                    .mean()\n",
    "        df_tasks['time_since_prev_task_1'] = df_tasks['timestamp'] - df_tasks.groupby('user_id')['timestamp'].shift()\n",
    "        #df_tasks['time_since_prev_task_2'] = df_tasks['timestamp'] - df_tasks.groupby('user_id')['timestamp'].shift(2)\n",
    "        \n",
    "        # Drop undesired columns\n",
    "        df_tasks.drop(columns=['timestamp'], inplace=True)\n",
    "        \n",
    "        # Join df_tasks to df_train\n",
    "        df = df.merge(df_tasks, on=['user_id', 'task_container_id'])\n",
    "    \n",
    "    else:\n",
    "        # Initialize user/content sums\n",
    "        user_sum = np.zeros(len(df), dtype=np.int16)\n",
    "        user_count = np.zeros(len(df), dtype=np.int16)\n",
    "        user_prev_sum = np.zeros(len(df), dtype=np.int16)\n",
    "        user_prev_count = np.zeros(len(df), dtype=np.int16)\n",
    "        user_prev_timestamp = np.zeros(len(df), dtype=np.int64)\n",
    "\n",
    "        # Get user historical info\n",
    "        for i, user_id in enumerate(df['user_id'].values):\n",
    "            user_sum[i] = user_sum_dict[user_id]\n",
    "            user_count[i] = user_count_dict[user_id]\n",
    "            user_prev_sum[i] = user_prev_task_sum_dict[user_id]\n",
    "            user_prev_count[i] = user_prev_task_count_dict[user_id]\n",
    "            user_prev_timestamp[i] = user_prev_task_timestamp_dict[user_id]\n",
    "            \n",
    "        # Add columns to df_test\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df['prev_task_cum_correctness'] = user_sum / user_count\n",
    "            df['prev_task_correctness'] = user_prev_sum / user_prev_count\n",
    "        df['prev_task_cum_count'] = user_count\n",
    "        df['time_since_prev_task_1'] = df['timestamp'] - user_prev_timestamp\n",
    "        \n",
    "        # Add task count\n",
    "        df['task_count'] = df.groupby('user_id')['user_id'].transform('count')\n",
    "        \n",
    "    # Content Columns - Not updating throughout testing phase\n",
    "    df['content_count'] = df['content_id'].map(content_agg['count']).astype('int32')\n",
    "    df['content_avg_correctness'] = df['content_id'].map(content_agg['sum'] / content_agg['count'])\n",
    "    \n",
    "    # Timestamp divided by number of tasks\n",
    "    df['timestamp_div_task_cum_count'] = df['timestamp'] / df['prev_task_cum_count']\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "df_train = clean(df_train\n",
    "                 ,target\n",
    "                 ,df_questions\n",
    "                 ,is_train=True\n",
    "                )\n",
    "\n",
    "# Capture user and content information\n",
    "content_agg = df_train.groupby('content_id')[target].agg(['sum', 'count'])\n",
    "user_agg = df_train.groupby('user_id')[target].agg(['sum', 'count'])\n",
    "\n",
    "# Get information on the last user task\n",
    "df_last_row = df_train[['user_id', 'task_container_id', 'timestamp']].groupby('user_id').tail(1)\n",
    "df_last_task = df_train.merge(df_last_row[['user_id', 'task_container_id']]\n",
    "                              , on=['user_id', 'task_container_id']\n",
    "                              , how='inner')\n",
    "\n",
    "if KAGGLE_RUN:\n",
    "    del df_train\n",
    "    gc.collect()   \n",
    "\n",
    "# This is for testing set\n",
    "user_prev_task = df_last_task\\\n",
    "            .groupby(['user_id', 'task_container_id'])[target]\\\n",
    "            .agg(['sum', 'count'])\\\n",
    "            .rename(columns={'sum': 'prev_task_sum', 'count': 'prev_task_count'})\\\n",
    "            .reset_index(level='task_container_id', drop=True)\n",
    "    \n",
    "# Feature engineer\n",
    "if not KAGGLE_RUN:\n",
    "    print('Starting FE')\n",
    "    df_train = do_fe(df_train, is_train=True, content_agg = content_agg)\n",
    "    \n",
    "    # Delete uneeded columns\n",
    "    df_train.drop(columns=['task_sum'\n",
    "                           ,'question_id'\n",
    "                           ,'content_id'\n",
    "                           ,'task_container_id'\n",
    "                           ,'content_type_id'\n",
    "                          ]\n",
    "                  ,inplace=True)\n",
    "    gc.collect()\n",
    "    print('Finished FE')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "features = [\n",
    "            # Task related\n",
    "            'prev_task_correctness'\n",
    "            ,'prev_task_cum_count'\n",
    "            ,'prev_task_cum_correctness'\n",
    "            ,'task_count'\n",
    "            ,'time_since_prev_task_1'\n",
    "            # Content related\n",
    "            ,'content_avg_correctness'\n",
    "            ,'content_count'\n",
    "            # From df_train\n",
    "            ,'timestamp_div_task_cum_count'\n",
    "            ,'timestamp'\n",
    "            ,'prior_question_elapsed_time'\n",
    "            ,'prior_question_had_explanation'\n",
    "            # From df_questions\n",
    "            ,'part'\n",
    "            ]\n",
    "\n",
    "if not KAGGLE_RUN:\n",
    "    pass\n",
    "    #assert(len(features) == (df_train.shape[1] - 2)) # Minuse 2 for target and user_id\n",
    "    \n",
    "if not KAGGLE_RUN:    \n",
    "    \n",
    "    # - # - # Make Validation Set # - # - #\n",
    "    # Add entire user histories to validation set\n",
    "    tmp = df_train.user_id.unique()\n",
    "    val_full_ids = np.random.choice(tmp, int(.05 * tmp.shape[0]))\n",
    "    df_valid_full = df_train.loc[df_train['user_id'].isin(val_full_ids)]\n",
    "    df_train.drop(df_valid_full.index, inplace=True)\n",
    "\n",
    "    # Add start of user histories to validation set\n",
    "    tmp = df_train.user_id.unique()\n",
    "    val_start_ids = np.random.choice(tmp, int(.07 * tmp.shape[0]))\n",
    "    df_valid_start = df_train.loc[df_train['user_id'].isin(val_start_ids)].groupby('user_id').head(100)\n",
    "    df_train.drop(df_valid_start.index, inplace=True)\n",
    "\n",
    "    # Add ending of user histories to validation set\n",
    "    tmp = df_train.user_id.unique()\n",
    "    val_end_ids = np.random.choice(tmp, int(.07 * tmp.shape[0]))\n",
    "    df_valid_end = df_train.loc[df_train['user_id'].isin(val_end_ids)].groupby('user_id').tail(100)\n",
    "    \n",
    "    # Clean up\n",
    "    df_train.drop(df_valid_end.index, inplace=True)\n",
    "    del tmp\n",
    "    del val_full_ids\n",
    "    del val_start_ids\n",
    "    del val_end_ids\n",
    "    gc.collect()\n",
    "    \n",
    "    # Combine the sets\n",
    "    print('Joining validation sets')\n",
    "    df_valid = df_valid_full.append([df_valid_start, df_valid_end])\n",
    "    \n",
    "    # - # - # Build and Save Model # - # - #\n",
    "    if True:\n",
    "        print('Building Model')\n",
    "        model = local_work.make_model(df_train, df_valid, target, features, learning_rate=1)\n",
    "        model.save_model(f'./output/' + model_name)\n",
    "        lgb.plot_importance(model, importance_type='gain')\n",
    "    else:\n",
    "        model = lgb.Booster(model_file='./output/' + model_name)\n",
    "    \n",
    "else:\n",
    "    model = lgb.Booster(model_file='/kaggle/input/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dicts\n",
    "user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "user_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "user_prev_task_sum_dict = user_prev_task['prev_task_sum']\\\n",
    "                            .astype('int16').to_dict(defaultdict(int))\n",
    "user_prev_task_count_dict = user_prev_task['prev_task_count']\\\n",
    "                            .astype('int16').to_dict(defaultdict(int))\n",
    "df_tmp = df_last_row.drop(columns=['task_container_id']).set_index('user_id')\n",
    "user_prev_task_timestamp_dict = df_tmp['timestamp']\\\n",
    "                                .astype('int64').to_dict(defaultdict(int))\n",
    "\n",
    "# Make env\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "df_prior_test = None\n",
    "\n",
    "# Make predictions\n",
    "for (df_test, df_sample_prediction) in iter_test:\n",
    "    \n",
    "    if df_prior_test is not None:\n",
    "        # Attach whether or not users answered correctly\n",
    "        df_prior_test[target] = eval(df_test['prior_group_answers_correct'].iloc[0])\n",
    "        df_prior_test = df_prior_test[df_prior_test[target] != -1].reset_index(drop=True)\n",
    "\n",
    "        user_ids = df_prior_test['user_id'].values\n",
    "        content_ids = df_prior_test['content_id'].values\n",
    "        targets = df_prior_test[target].values\n",
    "        \n",
    "        # If user present in current batch, reset the previous task dictionary\n",
    "        for user_id in df_test['user_id'].unique():\n",
    "            user_prev_task_sum_dict[user_id] = 0\n",
    "            user_prev_task_count_dict[user_id] = 0\n",
    "\n",
    "        # Update user and content dictionaries\n",
    "        for user_id, content_id, timestamp, answered_correctly in zip(df_prior_test['user_id'].values\n",
    "                                                                       ,df_prior_test['content_id'].values\n",
    "                                                                       ,df_prior_test['timestamp'].values\n",
    "                                                                       ,df_prior_test[target].values):\n",
    "            user_sum_dict[user_id] += answered_correctly\n",
    "            user_count_dict[user_id] += 1\n",
    "            user_prev_task_sum_dict[user_id] += answered_correctly\n",
    "            user_prev_task_count_dict[user_id] += 1\n",
    "            user_prev_task_timestamp_dict[user_id] = timestamp\n",
    "\n",
    "    # Make a copy of df_test for the next iteration\n",
    "    df_prior_test = df_test.copy()\n",
    " \n",
    "    # Clean\n",
    "    df_test = clean(df_test\n",
    "                     ,target\n",
    "                     ,df_questions\n",
    "                     ,is_train=False\n",
    "                    )\n",
    "    \n",
    "    # Feature engineer\n",
    "    df_test = do_fe(df_test\n",
    "                    ,is_train=False\n",
    "                    ,content_agg = content_agg\n",
    "                    ,user_sum_dict = user_sum_dict\n",
    "                    ,user_count_dict = user_count_dict\n",
    "                    ,user_prev_task_sum_dict = user_prev_task_sum_dict\n",
    "                    ,user_prev_task_count_dict = user_prev_task_count_dict\n",
    "                    ,user_prev_task_timestamp_dict = user_prev_task_timestamp_dict\n",
    "                    )\n",
    "    \n",
    "    # Make prediction\n",
    "    df_test[target] = model.predict(df_test[features])\n",
    "    env.predict(df_test[['row_id', target]])\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import riiideducation\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    all_dfs = []\n",
    "    for (df_test, df_sample_prediction) in iter_test:\n",
    "        all_dfs.append(df_test)\n",
    "        df_test[target] = .5\n",
    "        env.predict(df_test[['row_id', target]])\n",
    "\n",
    "    for i in range(3):\n",
    "        print(all_dfs[i].shape[0])\n",
    "        print(len(all_dfs[i].user_id.unique()) - all_dfs[i].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riiid",
   "language": "python",
   "name": "riiid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
