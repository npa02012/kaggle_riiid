{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether or not running on Kaggle\n",
    "import os\n",
    "KAGGLE_RUN = True\n",
    "if os.path.isdir('/home/ubuntu'):\n",
    "    KAGGLE_RUN = False\n",
    "    \n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "if not KAGGLE_RUN:\n",
    "    import sys\n",
    "    sys.path.insert(0, './input')\n",
    "    import local_work\n",
    "import riiideducation\n",
    "\n",
    "# Boto3 Setup and Download Files\n",
    "if not KAGGLE_RUN:\n",
    "    local_work = local_work.local_work()\n",
    "    local_work.download_riiid_files()\n",
    "\n",
    "# Set seed\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    \n",
    "# Define data for train.csv\n",
    "data_types_dict = {\n",
    "    'user_id': 'int32', \n",
    "    'content_id': 'int16', \n",
    "    'answered_correctly': 'int8', \n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool'\n",
    "}\n",
    "target = 'answered_correctly'\n",
    "\n",
    "# Load data\n",
    "if not KAGGLE_RUN:\n",
    "    df_train = local_work.get_train_data(data_types_dict)\n",
    "    df_questions = local_work.get_questions_data()\n",
    "else:\n",
    "    import datatable as dt\n",
    "    df_train = dt.fread('../input/riiid-test-answer-prediction/train.csv'\n",
    "                        ,columns=set(data_types_dict.keys())).to_pandas()\n",
    "    df_questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv'\n",
    "                                ,usecols=[0, 3]\n",
    "                                ,dtype={'question_id': 'int16', 'part': 'int8'}\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 8.11 µs\n",
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.58 µs\n",
      "CPU times: user 12 µs, sys: 4 µs, total: 16 µs\n",
      "Wall time: 22.2 µs\n",
      "CPU times: user 11 µs, sys: 4 µs, total: 15 µs\n",
      "Wall time: 20.5 µs\n",
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 46.3 µs\n",
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 8.82 µs\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 9.3 µs\n",
      "[LightGBM] [Info] Number of positive: 3602877, number of negative: 2933798\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3081\n",
      "[LightGBM] [Info] Number of data points in the train set: 6536675, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551179 -> initscore=0.205435\n",
      "[LightGBM] [Info] Start training from score 0.205435\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.751621\tvalid_1's auc: 0.734125\n",
      "[100]\ttraining's auc: 0.753068\tvalid_1's auc: 0.735803\n",
      "[150]\ttraining's auc: 0.753681\tvalid_1's auc: 0.736328\n",
      "[200]\ttraining's auc: 0.75403\tvalid_1's auc: 0.736534\n",
      "[250]\ttraining's auc: 0.754325\tvalid_1's auc: 0.736701\n",
      "[300]\ttraining's auc: 0.754591\tvalid_1's auc: 0.736824\n",
      "[350]\ttraining's auc: 0.75484\tvalid_1's auc: 0.73693\n",
      "[400]\ttraining's auc: 0.755067\tvalid_1's auc: 0.737001\n",
      "[450]\ttraining's auc: 0.755251\tvalid_1's auc: 0.737049\n",
      "[500]\ttraining's auc: 0.755457\tvalid_1's auc: 0.737093\n",
      "[550]\ttraining's auc: 0.755656\tvalid_1's auc: 0.737147\n",
      "[600]\ttraining's auc: 0.755858\tvalid_1's auc: 0.737188\n",
      "[650]\ttraining's auc: 0.756069\tvalid_1's auc: 0.737248\n",
      "[700]\ttraining's auc: 0.756242\tvalid_1's auc: 0.737279\n",
      "[750]\ttraining's auc: 0.756439\tvalid_1's auc: 0.737328\n",
      "[800]\ttraining's auc: 0.75662\tvalid_1's auc: 0.737354\n",
      "[850]\ttraining's auc: 0.756817\tvalid_1's auc: 0.737398\n",
      "[900]\ttraining's auc: 0.756979\tvalid_1's auc: 0.737414\n",
      "[950]\ttraining's auc: 0.757168\tvalid_1's auc: 0.737441\n",
      "[1000]\ttraining's auc: 0.757343\tvalid_1's auc: 0.737466\n",
      "[1050]\ttraining's auc: 0.757508\tvalid_1's auc: 0.737479\n",
      "[1100]\ttraining's auc: 0.757684\tvalid_1's auc: 0.73751\n",
      "[1150]\ttraining's auc: 0.757839\tvalid_1's auc: 0.737519\n",
      "[1200]\ttraining's auc: 0.757986\tvalid_1's auc: 0.737532\n",
      "[1250]\ttraining's auc: 0.758129\tvalid_1's auc: 0.73754\n",
      "[1300]\ttraining's auc: 0.75827\tvalid_1's auc: 0.737543\n",
      "[1350]\ttraining's auc: 0.758422\tvalid_1's auc: 0.737553\n",
      "[1400]\ttraining's auc: 0.758583\tvalid_1's auc: 0.73756\n",
      "[1450]\ttraining's auc: 0.75873\tvalid_1's auc: 0.737576\n",
      "[1500]\ttraining's auc: 0.758886\tvalid_1's auc: 0.737589\n",
      "[1550]\ttraining's auc: 0.759045\tvalid_1's auc: 0.737596\n",
      "[1600]\ttraining's auc: 0.759182\tvalid_1's auc: 0.737607\n",
      "[1650]\ttraining's auc: 0.759317\tvalid_1's auc: 0.737609\n",
      "[1700]\ttraining's auc: 0.759465\tvalid_1's auc: 0.737613\n",
      "[1750]\ttraining's auc: 0.759597\tvalid_1's auc: 0.737626\n",
      "[1800]\ttraining's auc: 0.759756\tvalid_1's auc: 0.737636\n",
      "[1850]\ttraining's auc: 0.759909\tvalid_1's auc: 0.737649\n",
      "[1900]\ttraining's auc: 0.760072\tvalid_1's auc: 0.737652\n",
      "Early stopping, best iteration is:\n",
      "[1884]\ttraining's auc: 0.760023\tvalid_1's auc: 0.737656\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train[target] != -1].reset_index(drop=True)\n",
    "df_train['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "df_train = df_train.astype(data_types_dict)\n",
    "\n",
    "\n",
    "df_train['lag'] = df_train.groupby('user_id')[target].shift()\n",
    "cum = df_train.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n",
    "df_train['user_correctness'] = cum['cumsum'] / cum['cumcount']\n",
    "df_train.drop(columns=['lag'], inplace=True)\n",
    "\n",
    "user_agg = df_train.groupby('user_id')[target].agg(['sum', 'count'])\n",
    "content_agg = df_train.groupby('content_id')[target].agg(['sum', 'count'])\n",
    "df_train = df_train.groupby('user_id').tail(24).reset_index(drop=True)\n",
    "   \n",
    "df_train = pd.merge(df_train, df_questions, left_on='content_id', right_on='question_id', how='left')\n",
    "df_train.drop(columns=['question_id'], inplace=True)\n",
    "\n",
    "df_train['content_count'] = df_train['content_id'].map(content_agg['count']).astype('int32')\n",
    "df_train['content_id'] = df_train['content_id'].map(content_agg['sum'] / content_agg['count'])\n",
    "\n",
    "df_valid = df_train.groupby('user_id').tail(6)\n",
    "df_train.drop(df_valid.index, inplace=True)\n",
    "\n",
    "features = [\n",
    "    'content_id',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation',\n",
    "    'user_correctness',\n",
    "    'part',\n",
    "    'content_count'\n",
    "]\n",
    "\n",
    "if not KAGGLE_RUN:\n",
    "    model = local_work.make_model(df_train, df_valid, target, features)\n",
    "    model.save_model(f'./output/test_model2.txt')\n",
    "    #lgb.plot_importance(model, importance_type='gain')\n",
    "else:\n",
    "    print('Loading Model and Making Predictions')\n",
    "    # Load model\n",
    "    model = lgb.Booster(model_file='/kaggle/input/riiid-test-model/test_model.txt')\n",
    "    \n",
    "    # Setup dicts\n",
    "    user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "    user_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "    content_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "    content_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "\n",
    "    # Make env\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    df_prior_test = None\n",
    "\n",
    "    # Make predictions\n",
    "    for (df_test, df_sample_prediction) in iter_test:\n",
    "        if df_prior_test is not None:\n",
    "            df_prior_test[target] = eval(df_test['prior_group_answers_correct'].iloc[0])\n",
    "            df_prior_test = df_prior_test[df_prior_test[target] != -1].reset_index(drop=True)\n",
    "\n",
    "            user_ids = df_prior_test['user_id'].values\n",
    "            content_ids = df_prior_test['content_id'].values\n",
    "            targets = df_prior_test[target].values\n",
    "\n",
    "            for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n",
    "                user_sum_dict[user_id] += answered_correctly\n",
    "                user_count_dict[user_id] += 1\n",
    "                content_sum_dict[content_id] += answered_correctly\n",
    "                content_count_dict[content_id] += 1\n",
    "\n",
    "        df_prior_test = df_test.copy()\n",
    "\n",
    "        df_test = df_test[df_test['content_type_id'] == 0].reset_index(drop=True)\n",
    "        df_test = pd.merge(df_test, df_questions, left_on='content_id', right_on='question_id', how='left')\n",
    "        df_test['prior_question_had_explanation'] = df_test['prior_question_had_explanation']\\\n",
    "                                                        .fillna(False).astype('bool')    \n",
    "\n",
    "        user_sum = np.zeros(len(df_test), dtype=np.int16)\n",
    "        user_count = np.zeros(len(df_test), dtype=np.int16)\n",
    "        content_sum = np.zeros(len(df_test), dtype=np.int32)\n",
    "        content_count = np.zeros(len(df_test), dtype=np.int32)\n",
    "\n",
    "        for i, (user_id, content_id) in enumerate(zip(df_test['user_id'].values, df_test['content_id'].values)):\n",
    "            user_sum[i] = user_sum_dict[user_id]\n",
    "            user_count[i] = user_count_dict[user_id]\n",
    "            content_sum[i] = content_sum_dict[content_id]\n",
    "            content_count[i] = content_count_dict[content_id]\n",
    "\n",
    "        df_test['user_correctness'] = user_sum / user_count\n",
    "        df_test['content_count'] = content_count\n",
    "        df_test['content_id'] = content_sum / content_count\n",
    "\n",
    "        df_test[target] = model.predict(df_test[features])\n",
    "        env.predict(df_test[['row_id', target]])\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riiid",
   "language": "python",
   "name": "riiid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
