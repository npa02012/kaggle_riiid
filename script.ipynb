{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 34969, number of negative: 28491\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2706\n",
      "[LightGBM] [Info] Number of data points in the train set: 63460, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551040 -> initscore=0.204874\n",
      "[LightGBM] [Info] Start training from score 0.204874\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.774488\tvalid_1's auc: 0.733889\n",
      "[100]\ttraining's auc: 0.787538\tvalid_1's auc: 0.733756\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's auc: 0.779495\tvalid_1's auc: 0.734152\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Decide whether or not running on Kaggle\n",
    "import os\n",
    "KAGGLE_RUN = True\n",
    "if os.path.isdir('/home/ubuntu'):\n",
    "    KAGGLE_RUN = False\n",
    "    \n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "if not KAGGLE_RUN:\n",
    "    import sys\n",
    "    sys.path.insert(0, './input')\n",
    "    import local_work\n",
    "import riiideducation\n",
    "\n",
    "# Boto3 Setup and Download Files\n",
    "if not KAGGLE_RUN:\n",
    "    local_work = local_work.local_work()\n",
    "    local_work.download_riiid_files()\n",
    "\n",
    "# Set seed\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    \n",
    "# Define data for train.csv\n",
    "data_types_dict = {\n",
    "    'user_id': 'int32', \n",
    "    'content_id': 'int16', \n",
    "    'answered_correctly': 'int8', \n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool'\n",
    "}\n",
    "target = 'answered_correctly'\n",
    "\n",
    "# Load data\n",
    "if not KAGGLE_RUN:\n",
    "    df_train = local_work.get_train_data()\n",
    "    df_questions = local_work.get_questions_data()\n",
    "else:\n",
    "    import datatable as dt\n",
    "    df_train = dt.fread('../input/riiid-test-answer-prediction/train.csv'\n",
    "                        ,columns=set(data_types_dict.keys())).to_pandas()\n",
    "    df_questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv'\n",
    "                                ,usecols=[0, 3]\n",
    "                                ,dtype={'question_id': 'int16', 'part': 'int8'}\n",
    "                               )\n",
    "\n",
    "df_train = df_train[df_train[target] != -1].reset_index(drop=True)\n",
    "df_train['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "df_train = df_train.astype(data_types_dict)\n",
    "\n",
    "df_train['lag'] = df_train.groupby('user_id')[target].shift()\n",
    "cum = df_train.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n",
    "df_train['user_correctness'] = cum['cumsum'] / cum['cumcount']\n",
    "df_train.drop(columns=['lag'], inplace=True)\n",
    "\n",
    "user_agg = df_train.groupby('user_id')[target].agg(['sum', 'count'])\n",
    "content_agg = df_train.groupby('content_id')[target].agg(['sum', 'count'])\n",
    "df_train = df_train.groupby('user_id').tail(24).reset_index(drop=True)\n",
    "\n",
    "df_train = pd.merge(df_train, df_questions, left_on='content_id', right_on='question_id', how='left')\n",
    "df_train.drop(columns=['question_id'], inplace=True)\n",
    "\n",
    "df_train['content_count'] = df_train['content_id'].map(content_agg['count']).astype('int32')\n",
    "df_train['content_id'] = df_train['content_id'].map(content_agg['sum'] / content_agg['count'])\n",
    "\n",
    "df_valid = df_train.groupby('user_id').tail(6)\n",
    "df_train.drop(df_valid.index, inplace=True)\n",
    "\n",
    "features = [\n",
    "    'content_id',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation',\n",
    "    'user_correctness',\n",
    "    'part',\n",
    "    'content_count'\n",
    "]\n",
    "\n",
    "if not KAGGLE_RUN:\n",
    "    model = local_work.make_model(df_train, df_valid, target, features)\n",
    "    model.save_model(f'./output/test_model2.txt')\n",
    "    #lgb.plot_importance(model, importance_type='gain')\n",
    "else:\n",
    "    print('Loading Model and Making Predictions')\n",
    "    # Load model\n",
    "    model = lgb.Booster(model_file='/kaggle/input/riiid-test-model/test_model.txt')\n",
    "    \n",
    "    # Setup dicts\n",
    "    user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "    user_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "    content_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "    content_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "\n",
    "    # Make env\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    df_prior_test = None\n",
    "\n",
    "    # Make predictions\n",
    "    for (df_test, df_sample_prediction) in iter_test:\n",
    "        if df_prior_test is not None:\n",
    "            df_prior_test[target] = eval(df_test['prior_group_answers_correct'].iloc[0])\n",
    "            df_prior_test = df_prior_test[df_prior_test[target] != -1].reset_index(drop=True)\n",
    "\n",
    "            user_ids = df_prior_test['user_id'].values\n",
    "            content_ids = df_prior_test['content_id'].values\n",
    "            targets = df_prior_test[target].values\n",
    "\n",
    "            for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n",
    "                user_sum_dict[user_id] += answered_correctly\n",
    "                user_count_dict[user_id] += 1\n",
    "                content_sum_dict[content_id] += answered_correctly\n",
    "                content_count_dict[content_id] += 1\n",
    "\n",
    "        df_prior_test = df_test.copy()\n",
    "\n",
    "        df_test = df_test[df_test['content_type_id'] == 0].reset_index(drop=True)\n",
    "        df_test = pd.merge(df_test, df_questions, left_on='content_id', right_on='question_id', how='left')\n",
    "        df_test['prior_question_had_explanation'] = df_test['prior_question_had_explanation']\\\n",
    "                                                        .fillna(False).astype('bool')    \n",
    "\n",
    "        user_sum = np.zeros(len(df_test), dtype=np.int16)\n",
    "        user_count = np.zeros(len(df_test), dtype=np.int16)\n",
    "        content_sum = np.zeros(len(df_test), dtype=np.int32)\n",
    "        content_count = np.zeros(len(df_test), dtype=np.int32)\n",
    "\n",
    "        for i, (user_id, content_id) in enumerate(zip(df_test['user_id'].values, df_test['content_id'].values)):\n",
    "            user_sum[i] = user_sum_dict[user_id]\n",
    "            user_count[i] = user_count_dict[user_id]\n",
    "            content_sum[i] = content_sum_dict[content_id]\n",
    "            content_count[i] = content_count_dict[content_id]\n",
    "\n",
    "        df_test['user_correctness'] = user_sum / user_count\n",
    "        df_test['content_count'] = content_count\n",
    "        df_test['content_id'] = content_sum / content_count\n",
    "\n",
    "        df_test[target] = model.predict(df_test[features])\n",
    "        env.predict(df_test[['row_id', target]])\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riiid",
   "language": "python",
   "name": "riiid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
